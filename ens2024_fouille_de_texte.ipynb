{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction à la fouille de texte en Python\n",
        "\n",
        "---\n",
        "\n",
        "Le dépôt Github durable de ce cours, ainsi que les textes et le code source, sont disponibles [à l'adresse suivante](https://github.com/paulhectork/cours_ens2024_fouille_de_texte/). Il contient aussi des références bibliographiques, et des liens vers d'autres tutoriels pour aller plus loin.\n",
        "\n",
        "---\n",
        "\n",
        "# Introduction\n",
        "\n",
        "Dans ce cours, nous allons faire une analyse statistique de trois romans de Virginia Woolf: *Mrs. Dalloway* (1925), *To the lighthouse* (1927), *The Waves* (1931). Il s'agit de voir si le style de Woolf évolue, et si oui, comment. On analyse les textes par \"lecture distante\" (par opposition à une lecture \"proche\", humaine): on ne *lit pas* les romans, mais on les prend comme corpus de données qui peuvent être approchées par des méthodes computationnelles. On peut donc travailler sur de grands corpus, ici de trois romans entiers:\n",
        "\n",
        "- **Mrs. Dalloway** décrit la journée de Clarissa Dalloway, une femme britannique de classe sociale supérieure, avant la 1e Guerre Mondiale. Le roman suit le flux de conscience de Clarissa, et il est souvent considéré comme une réponse à *Ulysses* de Joyce, qui suit lui aussi un personnage pendant une journée (livre qu'elle critique abondamment par ailleurs).\n",
        "- **To the lighthouse** décrit quelques journées de la famille Ramsay et de leurs invité.e.s dans leur maison de villégiature, sur l'île de Syke en Écosse. Le roman s'étale sur une période de 10 ans, avant et après la Première Guerre Mondiale. Il entrecroise différentes histoires familiales, contient peu de dialogues directs et se concentre sur les pensées des personnages.\n",
        "- **The Waves** est composé de solliloques et dialogues de six narrateur.ice.s qui s'étalent sur plusieurs années, de l'enfance à l'âge adulte. C'est généralement considéré comme son roman le plus expérimental, et le plus difficile à lire.\n",
        "\n",
        "Notre question de recherche:\n",
        "\n",
        "> Le style de Woolf évolue-t-il d'un roman à l'autre?\n",
        "> Est-ce que le caractère \"expérimental\" et \"difficile\" de *The Waves* ressort d'une analyse statistique ?\n",
        "\n",
        "Le choix du corpus, et une partie de la méthodologie, sont inspirés de:\n",
        "\n",
        "> Hussein, K. & Kadhim, R. (2020). A Corpus-Based Stylistic Identification of Lexical Density Profile of Three Novels by Virginia Woolf: The Waves, Mrs. Dalloway and To the Lighthouse. *International Journal of Psychosocial Rehabilitation*. 24. pp. 6688-9702. En accès libre à [cette addresse](https://www.researchgate.net/publication/343797320_A_Corpus-Based_Stylistic_Identification_of_Lexical_Density_Profile_of_Three_Novels_by_Virginia_Woolf_The_Waves_Mrs_Dalloway_and_To_the_Lighthouse)\n",
        "\n",
        "---\n",
        "\n",
        "# Le programme des festivités\n",
        "\n",
        "## Pipeline\n",
        "\n",
        "Ces trois romans ont été téléchargés depuis Archive.org en texte brut (c'est-à-dire, sans éléments de mise en page) et légèrement nettoyés (les en-têtes, fin de page, la pagination et la séparation de chapitres sont supprimés). La chaîne de traitement est la suivante:\n",
        "\n",
        "- **on ouvre les fichiers** et on lit leur contenu\n",
        "- **on les simplifie** rapidement\n",
        "- on découpe le texte en **liste de paragraphes** et on étudie la structure de chaque paragraphe\n",
        "- on découpe le texte en **liste de phrases** et on étudie la structure de chaque phrase\n",
        "- enfin, on étudie le **vocabulaire**: sa *densité lexicale*, sa distribution dans les trois textes.\n",
        "\n",
        "On commencera par des opérations basiques en Python, pour on finira par introduire quelques techniques de base en traitement automatisé du langage (TAL).\n",
        "\n",
        "## Compétences\n",
        "\n",
        "Notre analyse permet d'introduire au TAL et aux bases de l'utilisation de Python pour la fouille de texte:\n",
        "\n",
        "- **les bases de la syntaxe** Python\n",
        "- **manipulation des types de données basiques** en Python: chaînes de caractères, nombres entiers et décimaux, listes et dictionnaires\n",
        "- **expressions régulières** (`regex`) pour détecter des motifs dans du texte\n",
        "- **traitement automatisé du langage, avec `nltk`** (enfin, une très très très rapide introduction à quelques concepts de TAL)\n",
        "- **utilisation de librairies** Python\n",
        "- **calculs statistiques basiques**\n",
        "\n",
        "---\n",
        "\n",
        "# Poser les bases\n",
        "\n",
        "On commence par installer les **librairies** nécessaires. En Python, certaines fonctionnalités sont disponibles de base, mais la plupart sont organisées dans des libraires. Une librairie est un ensemble de fonctions rassemblées ensemble avec un but spécifique: visualisations, calcul... Certaines librairies sont disponibles de base en Python, d'autres sont développées par des tiers et doivent être installées. Une librairie peut aussi être appelée \"module\" ou \"package\".\n",
        "\n",
        "Les fichiers sont installés avec la commande `pip install <package name>` dans un terminal, ou `!pip install` dans un notebook."
      ],
      "metadata": {
        "id": "WhhkD8wsXaka"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcHPQ3RpXPtH"
      },
      "outputs": [],
      "source": [
        "!pip install click==8.1.7\n",
        "!pip install contourpy==1.1.1\n",
        "!pip install cycler==0.12.1\n",
        "!pip install fonttools==4.49.0\n",
        "!pip install importlib-resources==6.1.2\n",
        "!pip install joblib==1.3.2\n",
        "!pip install kiwisolver==1.4.5\n",
        "!pip install matplotlib==3.7.5\n",
        "!pip install nltk==3.8.1\n",
        "!pip install numpy==1.24.4\n",
        "!pip install packaging==23.2\n",
        "!pip install pillow==10.2.0\n",
        "!pip install pyparsing==3.1.1\n",
        "!pip install python-dateutil==2.9.0.post0\n",
        "!pip install pytz==2024.1\n",
        "!pip install regex==2023.12.25\n",
        "!pip install six==1.16.0\n",
        "!pip install tabulate==0.9.0\n",
        "!pip install tqdm==4.66.2\n",
        "!pip install tzdata==2024.1\n",
        "!pip install Unidecode==1.3.8\n",
        "!pip install zipp==3.17.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensuite, on importe les libraires nécessaires pour les utiliser dans notre chaîne de traitement."
      ],
      "metadata": {
        "id": "0D_44o-li2QS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from unidecode import unidecode\n",
        "from tabulate import tabulate\n",
        "import statistics\n",
        "import random\n",
        "import nltk\n",
        "import os\n",
        "import re"
      ],
      "metadata": {
        "id": "ZzNTH9hIjzHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour importer une librairie, on utilise la commande `import`:\n",
        "\n",
        "```python\n",
        "import <nom du paquet>\n",
        "```\n",
        "\n",
        "Les librairies sont souvent organisées en *sous-modules* (comme les chapitres d'un livre). Il est donc aussi possible d'importer seulement un *module*, ou une seule fonction:\n",
        "\n",
        "```python\n",
        "from <nom_du_paquet> import <nom_du_module>\n",
        "from <nom_du_paquet>.<nom_du_module> import <nom_du_module_ou_fonction>\n",
        "```\n",
        "\n",
        "Ensuite, `nltk` (la librairie faite pour le traitement du langage naturel) demande de faire quelques téléchargements:\n",
        "\n"
      ],
      "metadata": {
        "id": "xXQrWJuLj0Bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "BNOdTDhclYzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Lire les fichiers\n",
        "\n",
        "(Attention, il faut que les [trois fichiers](https://github.com/paulhectork/cours_ens2024_fouille_de_texte/tree/main/in) aient été téléchargés au format `.txt` dans la partie `Fichiers` de Google Collab, sans les renommer !)\n",
        "\n",
        "On commence donc par lire le contenu de nos textes."
      ],
      "metadata": {
        "id": "_VbOKuWillXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lire(nom_de_fichier):\n",
        "    with open(nom_de_fichier, mode=\"r\" ) as fh:\n",
        "        corpus = fh.read()\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "Dd-9i7WiJDsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qu'est-ce qu'il se passe au dessus??\n",
        "\n",
        "Pour **lire un fichier avec Python**, 2 étapes sont nécessaires: on ouvre le fichier, puis on lit son contenu et on l'assigne à une variable.\n",
        "\n",
        "- pour ouvrir, on utilise: `open(<nom_du_fichier>, mode=\"r\")`\n",
        "- `with open(...) as fh` permet d'ouvrir un fichier et de l'assigner à la variable `fh`. Le fichier est automatiquement fermé à la fin du bloc.\n",
        "- `fh.read()` permet de lire le contenu du fichier.\n",
        "\n",
        "Pour éviter d'avoir à réécrire du code, **on utilise des fonctions**.\n",
        "- Comme en maths, une fonction prend un ou plusieurs arguments, effectue des opérations et retourne une ou plusieurs valeurs.\n",
        "- `def` marque le début de la définition d'une fonction, `return` la fin.\n",
        "- `return` est optionnel. Ce mot-clé désigne le **résultat d'une fonction**. À la fin de l'exécusion d'une fonction, toutes les valeurs créés ou modifiées pendant l'exécution sont supprimées. Seules les valeurs retournées avec `return` peuvent être accédées en dehors de la fonction et utilisée ailleurs.\n",
        "- Une fonction peut être appelée plus tard (ça évite d'avoir à réécrire du code, la flemme étant un moteur majeur de lae développeur.euse)\n",
        "- Elle permet aussi de lancer la même opération avec différents arguments en entrée.\n",
        "\n",
        "```python\n",
        "def sommecarre(x,y): # sommecarre est le nom de la fonction, `x` le premier argument et `y` le 2e\n",
        "    a = x*x          # on fait une opération\n",
        "    b = y*y          # une deuxième opération\n",
        "    return  a+b      # on calcule la somme et on la retourne. `a` et `b` ne sont pas accessibles en dehors de la fonction\n",
        "z = sommecarre(3,7)  # on appelle la fonction. x=3 et y=7\n",
        "```"
      ],
      "metadata": {
        "id": "k03ebXOeJG44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# qu'est-ce qui se passe ici?\n",
        "dalloway = lire(\"./mrs_dalloway.txt\")\n",
        "lighthouse = lire(\"./to_the_lighthouse.txt\")\n",
        "waves = lire(\"./the_waves.txt\")\n",
        "\n",
        "print(waves)"
      ],
      "metadata": {
        "id": "Xt1_a82Tlgrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Préparer le texte\n",
        "\n",
        "Pour faciliter l'analyse, on a commencer par simplifier un peu le texte.\n",
        "\n",
        "En programmation, les données sont classées en différents \"types\", et chaque type de données permet certaines opérations, et vient avec des fonctions qui lui sont propres. On peut par exemple diviser un nombre entier, mais on ne peut pas diviser du texte. La fonction `type()` permet de connaître le type de donnée d'une variable.\n",
        "\n",
        "Notre texte est une chaîne de caractères (`string`) et des fonctions sont prédéfinies pour le manipuler. On les utilise ici:"
      ],
      "metadata": {
        "id": "pHcbcix6oup9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simplify(txt):\n",
        "    \"\"\"\n",
        "    simplifier les 3 romans\n",
        "    \"\"\"\n",
        "    txt = txt.lower() # supprimer les majuscules\n",
        "\n",
        "    # supprimer les points qui ne séparent pas 2 phrases\n",
        "    txt = txt.replace(\"mrs.\", \"mrs\")  # syntaxe: `replace(<texte à remplace>, <remplacement>)`\n",
        "    txt = txt.replace(\"ms.\", \"ms\")\n",
        "    txt = txt.replace(\"mr.\", \"mr\")\n",
        "    txt = txt.replace(\"dr.\", \"dr\")\n",
        "    txt = txt.replace(\"'s\", \"\")\n",
        "    # on peut l'écrire d'autres manières:\n",
        "    # txt = txt.replace(\"mrs.\", \"mrs\").replace(\"ms.\", \"ms\").replace(\"mr.\", \"mr\").replace(\"dr.\", \"dr\").replace(\"'s\", \"\")\n",
        "\n",
        "    # supprimer les accents des lettres accentuées\n",
        "    txt = unidecode(txt)  # unidecode n'existe pas par défaut, cette fonction a été installée avec la librairie `unidecode`\n",
        "\n",
        "    return txt\n",
        "\n",
        "waves = simplify(waves)\n",
        "dalloway = simplify(dalloway)\n",
        "lighthouse = simplify(lighthouse)\n",
        "\n",
        "print(lighthouse)\n"
      ],
      "metadata": {
        "id": "8F9yPrcsqW9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Diviser le texte en plus petites unités\n",
        "\n",
        "Nos textes sont des `string`. Un ordinateur lit une `string` de façon bête et méchante: caractère après caractère. Pour l'instant, on ne peut donc faire d'analyses qu'au niveau de l'ensemble du texte, alors qu'on voudrait travailler à des plus petites échelles: le paragraphe et la phrase. Il va donc falloir diviser ce texte en plus petites unités, à l'aide de **listes**.\n",
        "\n",
        "**Une liste est une série ordonnée de valeurs**.\n",
        "- Les items d'une liste sont séparés par des virgules (`,`)\n",
        "- Une liste et s'écrit entre `[]`. - Une liste peut contenir tout type de données: `string`, nombres entiers (`int`) et décimaux (`float`), mais aussi d'autres listes et des dictionnaires (`dict`, que l'on verra plus bas).\n",
        "- On accède aux items d'une liste sont par leur index, c'est-à-dire leur position dans la liste. L'indexation commence à 0: le premier item est à la position 0, le second à 1... C'est étrange au début mais on s'y fait."
      ],
      "metadata": {
        "id": "Jdehfi5prFrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sandwich = [ \"pain\", [\"tofu\", \"tomate séchée\", \"roquette\"], \"pain\" ]  # cette liste contient, dans l'ordre, une `string`, une `list`, une `string`\n",
        "print(sandwich[0])     # on accède au 1er élément\n",
        "print(sandwich[-1])    # si on fait `-n`, on accède au `nième` élement en partant de la fin de la liste. `-1` = le dernier élément de la liste !\n",
        "print(sandwich[1][1])  # le 2e élément du 2e élément de notre liste: \"tomate séchée\""
      ],
      "metadata": {
        "id": "lpzxXwREwE1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va donc écrire des fonctions pour scinder nos textes en listes de paragraphes et en listes de phrases. On commence par des petits bouts de code avant de tout rassembler dans des fonctions. Pour scinder un texte en liste, on utilise la méthode `.split(\"<caractère>\")`."
      ],
      "metadata": {
        "id": "TsdIIqMGw8FQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(waves.split(\" \")[:10])  # une liste des 10 premiers mots de The Waves"
      ],
      "metadata": {
        "id": "PitnAWi4w_qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour faire **une liste de paragraphes**, on `split` le texte en lignes vides (en informatique, `\\n` est un saut de ligne, `\\n\\n` une ligne vide)."
      ],
      "metadata": {
        "id": "ikMn0u5VxXv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dalloway.split(\"\\n\\n\")[3])         # qu'est-ce qu'on affiche ici?\n",
        "print(lighthouse.split(\"\\n\\n\")[10:-10])  # et ici?"
      ],
      "metadata": {
        "id": "CsF_-NrKx653"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour faire **une liste de phrases**, c'est un peu plus compliqué. Comme on a simplifié le texte, un `.` représente toujours une fin de phrase. Mais `?` et `!` sont aussi des fins de phrases. Il y a donc plusieurs séparateurs à détecter.\n",
        "\n",
        "On va donc utiliser une **expression régulière** (regex pour les intimes). Les regex sont très répandues en informatique, et offrent une syntaxe très puissante (mais indigeste) pour détecter des motifs dans du texte.\n",
        "- L'idée est de ne pas s'intéresser au contenu d'un texte, mais à sa structure: par exemple, 02/05/1972 et 10/12/1999 correspondent au motif: `[2 chiffres]/[2chiffres]/[4 chiffres]`. Il est alors possible de cibler ces deux dates par le même motif.\n",
        "- Les regex définissent (entre autres):\n",
        "  - **des quantificateurs**: ils permettent de quantifier le nombre de fois que l'on cherche un caractère:\n",
        "    - `x?`: 0 ou 1 fois `x`,\n",
        "    - `x+`: 1 ou plusieurs fois `x`,\n",
        "    - `x*`: de 0 à n fois x...\n",
        "  - **des classes de caractères**:\n",
        "    - `\\d`, par exemple, représente \"n'importe quel chiffre\";\n",
        "    - `\\s` est un espace\n",
        "    - `.` est n'importe quel caractère;\n",
        "    - `^` est le début d'une chaîne de caractères\n",
        "    - `$` est la fin d'une chaîne de caractère.\n",
        "  - **des opérateurs d'aleternance** (\"caractère A ou B\" correspond à: `[xy]` ou `(x|y)`)\n",
        "- *pour une présentation plus détaillée, [voir ici](https://github.com/paulhectork/cours_ens2023_fouille_de_texte/blob/main/1_fouille_texte.ipynb)*\n",
        "\n",
        "On veut scinder notre texte par `.`, `?` ou `!`. On utilise donc l'opérateur d'alternance `[]`, et on obtient la regex: `[\\.?!]` (dans une regex, le point doit être précédé de `\\`, sinon `.` est interprété comme *n'importe quel caractère*).\n",
        "\n",
        "Pour *splitter* avec une regex, on utilise la fonction `split()` de la librairie `re` (librairie consacrée aux regex):"
      ],
      "metadata": {
        "id": "4j-Us7Ckyig8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6SRxBgpW1cQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(re.split(\"[\\.?!]\", waves)[:10])       # qu'est-ce que j'affiche?\n",
        "print(len( re.split(\"[\\.?!]\", dalloway) ))  # len() permet donne la longueur d'une liste ! comment interpréter ce résultat ?"
      ],
      "metadata": {
        "id": "7CokuDQ11KOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On combine tout ce qu'on a vu en une fonction, `splitter()`, qui prend en argument `txt` un texte (chaîne de caractère) et retourne deux listes: une liste de paragraphes et une liste de phrases."
      ],
      "metadata": {
        "id": "uRcDFW7E2ffW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def splitter(txt):\n",
        "    \"\"\"\n",
        "    diviser le texte en unités distinctes: paragraphes et phrases.\n",
        "    prend `txt`, une chaîne de caractères en entrées\n",
        "    retourne `txt_paragraphe`, une liste de tous les paragraphes de ce texte\n",
        "             et `txt_phrase`, une liste de toutes les phrases de ce texte\n",
        "    \"\"\"\n",
        "    txt_paragraphe = []\n",
        "    splitted = txt.split(\"\\n\\n\")       # `.split()` produit ici une liste de paragraphes)\n",
        "    for t in splitted:                 # itérer sur chaque paragraphe. `t` = string contenant un paragraphe\n",
        "        if not re.search(\"^\\s*$\", t):  # ne pas prendre en compte les paragraphes vides\n",
        "            txt_paragraphe.append(t)   # ajouter l'item à la liste\n",
        "\n",
        "    txt_phrase = []\n",
        "    txt = txt.replace(\"\\n\", \" \")       # on supprime les sauts de ligne\n",
        "    for t in re.split(\"[\\.?!]\", txt):  # re.split() permet de séparer une chaîne de caractères en listes en utilisant une regex. ici, `[\\.?!]`, c'est à dire \".\" ou \"?\" ou \"!\"\n",
        "        if not re.search(\"^\\s*$\", t):\n",
        "            txt_phrase.append(t)\n",
        "\n",
        "    return txt_paragraphe, txt_phrase\n",
        "\n",
        "waves_paragraphe, waves_phrase = splitter(waves)\n",
        "dalloway_paragraphe, dalloway_phrase = splitter(dalloway)\n",
        "lighthouse_paragraphe, lighthouse_phrase = splitter(lighthouse)\n",
        "\n",
        "print(waves_paragraphe[10])\n",
        "print(lighthouse_phrase[-3:])"
      ],
      "metadata": {
        "id": "7wlrB5gP2sCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Étudier les paragraphes\n",
        "\n",
        "On a donc un texte nettoyé (`waves`, `dalloway`, `lighthouse`) et scindé en paragraphes (`*_paragraphe`) et en phrases `*_phrase`). On va donc pouvoir commencer notre analyse !\n",
        "\n",
        "Tous les résultats seront stockés dans le type de données **dictionnaire (`dict`)**.\n",
        "- Comme dans un vrai dictionnaire où un mot est associé à sa définition, un dictionnaire est un type de donnée permettant d'associer **une clé à une valeur**.\n",
        "- Sa **syntaxe** est: `{ \"clé1\": <valeur 1>, \"clé2\": <valeur2> }`.\n",
        "- Comme une liste, un dictionnaire peut contenir en valeurs n'importe quel type de données: `string`, `list`, autres `dict`... On peut donc représenter des objets très complexes en dictionnaires.\n",
        "- On accède aux valeurs à partir des clés: `dico[\"clé1\"]`."
      ],
      "metadata": {
        "id": "3iUSvBs_6qPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sandwich = {\n",
        "    \"pain\": { \"type\":\"pain pita\", \"quantité\": 2 },\n",
        "    \"ingrédients\": [ \"houmous\", \"falafel\", \"aubergines\"\n",
        "                   , \"zestes de citron\", \"coriandre\" ]\n",
        "}\n",
        "print(sandwich[\"pain\"])              # la valeur associée à `pain`\n",
        "print(sandwich[\"pain\"][\"quantité\"])  # sandwich -> pain -> quantité\n",
        "print(sandwich[\"ingrédients\"][3])    # et là, qu'est-ce qui s'affiche?"
      ],
      "metadata": {
        "id": "7XfPKq306n21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On créé donc 3 dictionnaires vides pour stocker les résultats de notre analyse des trois romans."
      ],
      "metadata": {
        "id": "38yunAYx9kTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "waves_stats = {}\n",
        "dalloway_stats = {}\n",
        "lighthouse_stats = {}"
      ],
      "metadata": {
        "id": "YtGHnABs9pV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On analyse la structure de nos paragraphes:\n",
        "- quel est le nombre médian de mots par paragraphes?\n",
        "- quel est le nombre médian de phrases par paragraphes?\n",
        "\n",
        "Le nombre médian de mots par paragraphes, c'est facile:"
      ],
      "metadata": {
        "id": "l68bCcYn9065"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_mots = []                 # liste contenant le nombre de mots pour chaque paragraphe du texte\n",
        "for p in dalloway_paragraphe:   # on accède à chaque paragraphe de mrs. dalloway\n",
        "  p = p.split(\" \")              # on fait du paragraphe une liste de mots\n",
        "  nb_mots = len(p)              # la longueur de la liste de mots = le nombre de mots du paragraphe\n",
        "  count_mots.append(nb_mots)\n",
        "print(statistics.median(count_mots))  # statistics.median() permet de calculer la valeur médiane d'une liste de nombres"
      ],
      "metadata": {
        "id": "fpsOeoeo-OkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour calculer le nombre médian de phrases par paragraphes, une petite subtilité: on devra supprimer les paragraphes vides (c'est-à-dire, qui ne contiennent que des espaces ou rien):"
      ],
      "metadata": {
        "id": "hYPIziVv-rEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_phrases = []                      # liste du nombre de phrases par paragraphe\n",
        "for p in dalloway_paragraphe:\n",
        "  nb_phrases = 0\n",
        "  p = re.split(\"[\\.?!]\", p)             # on fait une liste de paragraphes\n",
        "  for phrase in p:\n",
        "    if not re.search(\"^\\s*$\", phrase):  # que veut dire cette ligne?\n",
        "      nb_phrases += 1                   # on incrémente le compteur\n",
        "  count_phrases.append(nb_phrases)\n",
        "print(statistics.median(count_phrases))\n"
      ],
      "metadata": {
        "id": "qGBq_fg4_mO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On a calculé le nombre médian de mots par paragraphes et le nombre médian de phrases par paragraphe: il ne reste plus qu'à faire une fonction pour faire les calculs sur les 3 romans !"
      ],
      "metadata": {
        "id": "Ni1qGTQuBC8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def study_paragraphe(paragraphes, stats):\n",
        "    \"\"\"\n",
        "    analyser la structure d'un paragraphe:\n",
        "    longueur médiane d'un paragraphe en nombres de mots et en nombre de phrases\n",
        "\n",
        "    :param paragraphes: une liste contenant tous les paragraphes d'un texte\n",
        "    :param stats      : le dictionnaire contenant les statistiques sur un texte\n",
        "    \"\"\"\n",
        "    count_mots = []     # liste avec le nombre de mots pour chaque paragraphe\n",
        "    count_phrases = []  # liste du nombre de mots phrases par paragraphe\n",
        "\n",
        "    # on calcule le nombre médian de mots par paragraphe\n",
        "    for p in paragraphes:\n",
        "        p = p.split(\" \")           # on transforme le paragraphe en une liste de mots\n",
        "        count_mots.append(len(p))  # on ajoute à `counts_mots` `len(p)`, soit le nombre d'items dans la liste `p`\n",
        "    med_mots = statistics.median(count_mots)\n",
        "\n",
        "    # nombre médian de phrases par paragraphes\n",
        "    for p in paragraphes:\n",
        "        nb_phrases = 0\n",
        "        p = re.split(\"[\\.?!]\", p)               # on fait une liste de paragraphes\n",
        "        for phrase in p:\n",
        "            if not re.search(\"^\\s*$\", phrase):  # on ne prend pas en compte les lignes vides\n",
        "                nb_phrases += 1                 # on incrémente le compteur\n",
        "        count_phrases.append(nb_phrases)\n",
        "    med_phrases = statistics.median(count_phrases)\n",
        "\n",
        "    print(med_mots, med_phrases)\n",
        "    stats[\"nombre médian de mots par paragraphes\"] = med_mots\n",
        "    stats[\"nombre médian de phrases par paragraphes\"] = med_phrases\n",
        "    return stats\n",
        "\n",
        "stats_waves = study_paragraphe(waves_paragraphe, waves_stats)\n",
        "stats_dalloway = study_paragraphe(dalloway_paragraphe, dalloway_stats)\n",
        "stats_lighthouse = study_paragraphe(lighthouse_paragraphe, lighthouse_stats)\n",
        "print(\"the waves: nombre médian de mots par paragraphes    : \", waves_stats[\"nombre médian de mots par paragraphes\"])\n",
        "print(\"the waves: nombre médian de phrases par paragraphes : \", waves_stats[\"nombre médian de phrases par paragraphes\"])\n"
      ],
      "metadata": {
        "id": "NUQTZxw1BPua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Étudier les phrases\n",
        "\n",
        "On a fini la première échelle de notre étude: l'étude paragraphe par paragraphe :)) On va donc pouvoir commencer à étudier la structure des phrases dans les trois romans. On va s'intéresser à deux indicateurs:\n",
        "\n",
        "- le nombre médian de mots par phrases (une variante de ce qu'on a fait plus haut)\n",
        "- le nombre moyen de signes de ponctuations par phrases. Cet indicateur permettra de regarder comment évolue la complexité des phrases, mais aussi à approximer le nombre de propositions dans une phrase. On pourra donc voir l'évolution de la complexité d'une phrase.\n",
        "  - Ici, on préfère la moyenne à la médiane parce que la médiane est déséquilibrée par le grand nombre de phrases sans signes de ponctuation. La moyenne permet mieux de représenter ce qui se passe au niveau des phrases plus complexes, avec plusieurs signes de ponctuation.\n",
        "\n",
        "On commence par le nombre médian de mots par phrases:"
      ],
      "metadata": {
        "id": "5nFEWgA2CqW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_mots = []\n",
        "for p in dalloway_phrase:\n",
        "    counter = 0\n",
        "    p = p.split(\" \")                      # on fait de `p` une liste de mots\n",
        "    for mot in p:\n",
        "        if not re.search(\"^\\s*$\", mot):   # on ne compte pas les éléments vides de la liste de mots\n",
        "            counter += 1\n",
        "    count_mots.append(counter)\n",
        "med_mots = statistics.median(count_mots)\n",
        "print(med_mots)  # nombre médian de mots par phrases pour Mrs. Dalloway\n"
      ],
      "metadata": {
        "id": "LV1D4EphDlLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensuite, on calcule le nombre moyen de signes de ponctuation par phrase, pour un roman. La logique est similaire à ce qu'on a fait avant:\n",
        "\n",
        "- on intère sur chaque phrase d'un roman\n",
        "- on extrait les chiffres pertinents pour cette phrase\n",
        "- on l'ajoute à `count_punct`, une liste contenant toutes les données pour chaque phrase\n",
        "- on calcule une moyenne de toutes les valeurs de `count_punct`.\n",
        "\n",
        "Pour reprérer les signes de ponctuation, on utilise la regex suivante:\n",
        "\n",
        "```\n",
        "(-{2,}|[,;:&—\\(])\n",
        "```\n",
        "- `(<a>|<b>)`: soit `<a>`, soit `<b>`\n",
        "  - `<a>` = `-{2,}`: deux tirets ou plus (pour représenter un cadratin)\n",
        "  - `<b>` = `[,;:&—\\(]`: un de ces signes de ponctuation. (`(` est précédé d'un `\\` car c'est un caractère spécial dans une regex).\n",
        "- on cherche donc soit deux tirets, soit un des signes de ponctuation entre deux crochets.\n",
        "- on utilise `re.findall()`, qui permet d'obtenir une liste de toutes les occurrences trouvées dans une `string`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vcmH1VrhFXuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_punct = []\n",
        "for p in waves_phrase:\n",
        "    punct = re.findall(\"([,;:&—\\(]|-{2,})\", p)\n",
        "    count_punct.append(len(punct))      # on ajoute le nb de signes de ponctuations pour chaque phrase\n",
        "moyenne = statistics.mean(count_punct)\n",
        "print( round(moyenne, 3) )  # `round()` donne l'arrondi d'un nombre décimal. le 1e argument est le nombre à arrondir, le 2e argument est le nombre de chiffres après la virgules\n"
      ],
      "metadata": {
        "id": "FdMsSANeHokl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nos opérations de calcul sont prêtes ! On peut donc les combiner dans une chouette fonction:\n"
      ],
      "metadata": {
        "id": "3AqUOOWwIMyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def study_phrase(phrases, stats):\n",
        "    \"\"\"\n",
        "    analyser la structure d'une phrase: longueur médiane\n",
        "    d'une phrase, nombre de signes de ponctuation\n",
        "    \"\"\"\n",
        "    # nombre médian de mots par phrases\n",
        "    count_mots = []\n",
        "    for p in phrases:\n",
        "        p = p.split(\" \")                                    # on fait de `p` une liste de mots\n",
        "        p = [ x for x in p if not re.search(\"^\\s*$\", x) ]   # on enlève les éléments vides de la liste\n",
        "        count_mots.append(len(p))                           # on compte le nombre de mots dans la liste et on les ajoute à notre compteur\n",
        "    med_mots = statistics.median(count_mots)\n",
        "\n",
        "    # moyenne de signes de ponctuation par phrases.\n",
        "    count_punct = []\n",
        "    for p in phrases:\n",
        "        punct = re.findall(\"([,;:&—\\(]|-{2,})\", p)          # re.findall() retourne une liste de toutes les occurences de la regex trouvées\n",
        "        count_punct.append(len(punct))\n",
        "    mean_punct = round(statistics.mean(count_punct), 3)\n",
        "\n",
        "    stats[\"nombre médian de mots par phrase\"] = med_mots\n",
        "    stats[\"nombre moyen de signes de ponctuation par phrase\"] = mean_punct\n",
        "    return stats\n",
        "\n",
        "stats_waves = study_phrase(waves_phrase, waves_stats)\n",
        "stats_dalloway = study_phrase(dalloway_phrase, dalloway_stats)\n",
        "stats_lighthouse = study_phrase(lighthouse_phrase, lighthouse_stats)\n",
        "\n",
        "print(\"to the lighthouse: nombre médian de mots par phrase                 : \", stats_lighthouse[\"nombre médian de mots par phrase\"])\n",
        "print(\"to the lighthouse: nombre moyen de signes de ponctuation par phrase : \", stats_lighthouse[\"nombre moyen de signes de ponctuation par phrase\"])\n"
      ],
      "metadata": {
        "id": "2Nr9ByMxIaKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Étudier le vocabulaire: la *densité lexicale*\n",
        "\n",
        "## Définitions\n",
        "\n",
        "Jusqu'à maintenant, on est resté sur des méthodes d'analyse assez basiques, centrées sur la détection de motifs dans le texte. Ça nous a permis d'introduire quelques bases de Python, mais on est assez loin de produire quelque chose de vraiment intéressant.\n",
        "\n",
        "À partir de maintenant, on va utiliser des méthodes de TAL, avec la librairie Python dédiée, `nltk`. Il existe tout plein de méthodes de traitement du langage (reconnaissance d'entités nommées, *topic modelling*, *sentiment analysis*...)\n",
        "\n",
        "Ici, on va s'intéresser à la [densité lexicale](https://en.wikipedia.org/wiki/Lexical_density) de chaque texte. La densité lexicale vise à mesurer la complexité informationnelle d'un texte, écrit ou oral. Elle s'appuie sur la classification des mots en deux catégories:\n",
        "\n",
        "- les **mots-lexicaux** (`lexical units`) regroupent l'ensemble de mots \"porteurs d'information\" dans un texte: les noms, les verbes, les adverbes et les adjectifs qualitatifs. On considère que c'est grâce à eux que l'information est transmise.\n",
        "- les **mots-outils ou mots-grammaticaux** (`function words`) regroupent tout le reste des mots. On considère qu'ils sont moins porteurs de sens et qu'ils servent surtout à la structure de la phrase.\n",
        "\n",
        "La classification entre ces deux groupes n'est pas clairement définie et peut varier d'une étude à l'autre.\n",
        "\n",
        "La **densité lexicale** (notée $L_d$) mesure, sur une échelle de 0 à 100, la proportion de mots lexicaux dans un texte. Plus $L_d$ est proche de 100, plus il y a de mots lexicaux, plus on considère que le texte est riche en information. On considère en moyenne que que:\n",
        "- pour un texte écrit, $L_d > 40$, et pour un texte oral, $L_d < 40$.\n",
        "- pour un texte de fiction, $ 40 < L_d < 54$; pour un texte de non-fiction, $40 < L_d < 65$.\n",
        "\n",
        "Elle se mesure ainsi (avec $N_{lex}$ le nombre de mots lexicaux et $N$ le nombre total de mots dans un texte:\n",
        "\n",
        "$L_d = (\\frac{N_{lex}}{N}) \\times 100$\n",
        "\n",
        "PS: il existe plusieurs variantes de calcul, mais on utilise celle définie par Ure J. (1971).\n",
        "\n",
        "## Chaîne de traitement\n",
        "\n",
        "Avec `nltk`, calculer une densité lexicale est vraiment très très simple:\n",
        "\n",
        "- on tokenise notre texte en `tokens` (items lexicaux)\n",
        "- on fait un [`POS tag`](https://fr.wikipedia.org/wiki/%C3%89tiquetage_morpho-syntaxique) (*part-of-speech tagging*) pour déterminer la fonction de chaque *token* dans le texte: pronom, verbe...\n",
        "- on compte le nombre d'occurrences pour chaque fonction des tokens d'un texte\n",
        "- à partir de là, on calcule notre $L_d$\n",
        "\n",
        "Les tags possibles pour notre étiquetage morpho-syntaxique sont:\n",
        "\n",
        "```\n",
        "Tag  | Meaning             | English Examples\n",
        "~~~~~|~~~~~~~~~~~~~~~~~~~~~|~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
        "ADJ  | adjective           | new, good, high, special, big, local\n",
        "ADP  | adposition          | on, of, at, with, by, into, under\n",
        "ADV  | adverb              | really, already, still, early, now\n",
        "CONJ | conjunction         | and, or, but, if, while, although\n",
        "DET  | determiner, article | the, a, some, most, every, no, which\n",
        "NOUN | noun                | year, home, costs, time, Africa\n",
        "NUM  | numeral             | twenty-four, fourth, 1991, 14:24\n",
        "PRT  | particle            | at, on, out, over per, that, up, with\n",
        "PRON | pronoun             | he, their, her, its, my, I, us\n",
        "VERB | verb                | is, say, told, given, playing, would\n",
        ".    | punctuation marks   | . , ; !\n",
        "X    | other               | ersatz, esprit, dunno, gr8, univeristy\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Njf7kOJvPyHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize(waves)              # tokenisation au mot (similaire à txt.split(\" \"), mais performe des simplifications en plus)\n",
        "print(tokens)\n",
        "\n",
        "size = len(tokens)                              # on travaille sur tout le corpus\n",
        "pos = nltk.pos_tag(tokens, tagset=\"universal\")  # part-of-speech tagging. universal définit des classes très généralistes\n",
        "print(pos[:10])                                 # que voit-on ici?\n",
        "tags = []\n",
        "for (token, tag) in pos:\n",
        "    tags.append(tag)\n",
        "\n",
        "fd = nltk.FreqDist(tags)  # `nltk.FreqDist()` associe à chaque valeur distincte d'une liste le nombre d'occurrences dans cette liste\n",
        "fd.tabulate()\n",
        "\n",
        "nlex = fd.get(\"NOUN\") + fd.get(\"VERB\") + fd.get(\"ADJ\") + fd.get(\"ADV\")  # nb d'unités lexicales\n",
        "ld = 100 * (nlex/size)"
      ],
      "metadata": {
        "id": "AAUDJhUhX6yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On écrit une fonction qui reprenne tout ça et on la lance:"
      ],
      "metadata": {
        "id": "QGswVFZSeEEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def densite_lexicale(txt, stats):\n",
        "    \"\"\"\n",
        "    enfin, on étudie la densité lexicale de chaque roman\n",
        "\n",
        "    on suit la méthode de Ure: 100 * <nb d'unités lexicales> / <nb de tokens>\n",
        "    https://en.wikipedia.org/wiki/Lexical_density\n",
        "    https://www.nltk.org/book/ch05.html\n",
        "    \"\"\"\n",
        "    tokens = nltk.word_tokenize(txt)    # tokenisation au mot (similaire à txt.split(\" \"), mais performe des simplifications en plus)\n",
        "\n",
        "    size = len(tokens)  # on travaille sur tout le corpus\n",
        "    pos = nltk.pos_tag(tokens, tagset=\"universal\")  # part-of-speech tagging (classification du texte en classes: verbes...). universal définit des classes très généralistes\n",
        "    tags = []\n",
        "    for (token, tag) in pos:\n",
        "        tags.append(tag)\n",
        "\n",
        "    fd = nltk.FreqDist(tags)  # valeur associée aux nombre d'occurrences de celle-ci\n",
        "    nlex = fd.get(\"NOUN\") + fd.get(\"VERB\") + fd.get(\"ADJ\") + fd.get(\"ADV\")  # nb d'unités lexicales\n",
        "    ld = 100 * (nlex/size)\n",
        "    print(ld)\n",
        "\n",
        "    stats[\"densité lexicale\"] = round(ld, 3)\n",
        "\n",
        "    return stats\n",
        "\n",
        "stats_lighthouse = densite_lexicale(lighthouse, lighthouse_stats)\n",
        "stats_dalloway = densite_lexicale(dalloway, dalloway_stats)\n",
        "stats_waves = densite_lexicale(waves, stats_waves)\n"
      ],
      "metadata": {
        "id": "qZt2MfIveDR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Afficher les résultats\n",
        "\n",
        "La base de notre analyse est finie ! Il ne reste plus qu'à afficher les résultats:)\n",
        "\n",
        "Pour ce faire, on utilise la fonction `tabulate.tabulate()`, qui prend deux arguments: `headers` (une liste d'en-têtes) et `data` (une liste de listes, avec une liste interme par ligne)."
      ],
      "metadata": {
        "id": "vQkZWRRvs3Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headers = [ \"\", \"Mrs. Dalloway, 1925\", \"To the Lighthouse, 1927\", \"The Waves, 1931\" ]\n",
        "data = [ [ \"nombre médian de mots par paragraphes\"\n",
        "         , stats_dalloway[\"nombre médian de mots par paragraphes\"]\n",
        "         , stats_lighthouse[\"nombre médian de mots par paragraphes\"]\n",
        "         , stats_waves[\"nombre médian de mots par paragraphes\"]\n",
        "         ],\n",
        "         [ \"nombre médian de phrases par paragraphes\"\n",
        "         , stats_dalloway[\"nombre médian de phrases par paragraphes\"]\n",
        "         , stats_lighthouse[\"nombre médian de phrases par paragraphes\"]\n",
        "         , stats_waves[\"nombre médian de phrases par paragraphes\"]\n",
        "         ],\n",
        "         [ \"nombre médian de mots par phrase\"\n",
        "         , stats_dalloway[\"nombre médian de mots par phrase\"]\n",
        "         , stats_lighthouse[\"nombre médian de mots par phrase\"]\n",
        "         , stats_waves[\"nombre médian de mots par phrase\"]\n",
        "         ],\n",
        "         [ \"nombre moyen de signes de ponctuation par phrase\"\n",
        "         , stats_dalloway[\"nombre moyen de signes de ponctuation par phrase\"]\n",
        "         , stats_lighthouse[\"nombre moyen de signes de ponctuation par phrase\"]\n",
        "         , stats_waves[\"nombre moyen de signes de ponctuation par phrase\"]\n",
        "         ],\n",
        "         [ \"densité lexicale\"\n",
        "         , stats_dalloway[\"densité lexicale\"]\n",
        "         , stats_lighthouse[\"densité lexicale\"]\n",
        "         , stats_waves[\"densité lexicale\"]\n",
        "         ]\n",
        "]\n",
        "print(tabulate(data, headers, tablefmt=\"rounded_grid\"))"
      ],
      "metadata": {
        "id": "liiF2LBEtPc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qu'est-ce qu'on peut dire de ces résultats?"
      ],
      "metadata": {
        "id": "Ph8X0p6Ptqno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# (Bonus?) Étudier le vocabulaire: distribution du vocabulaire\n",
        "\n",
        "Pour finir, on va étudier la **distribution du vocabulaire** dans les texte. **On classe les tokens en déciles**, des mots les plus utilisés aux mots les moins utilisés. Le but, c'est de voir si l'autrice utilise beaucoup de mots \"rares\", ou si au contraire son vocabulaire se constitue au contraire d'une classe de mots qu'elle répère très souvent.\n",
        "\n",
        "On va calculer deux mesures:\n",
        "\n",
        "- la moyenne d'occurrences d'un item dans un texte, par décile\n",
        "- le nombre d'items distincts par un décile\n",
        "\n",
        "Le processus est le suivant:\n",
        "\n",
        "1. supprimer les **caractères non-alphabétiques**\n",
        "2. tokeniser le texte\n",
        "3. supprimer les **\"stopwords\"** (ensemble de mots souvent présents qui déséquilibrent l'étude du vocabulaire)\n",
        "4. faire un **`pos tagging`** et créer un **`sample` de 5000 mots-contenu** pour chaque texte\n",
        "5. **lémmatiser** notre sample. La [lémmatisation](https://fr.wikipedia.org/wiki/Lemmatisation), c'est extraire le *lemme*, c'est-à-dire la forme canonique, d'un mot. Par exemple: `était` -> `être`\n",
        "6. calculer une **distrubtion de fréquence** de nos lemmes\n",
        "7. **classer notre corpus en déciles**. Pour chaque décile, calculer le nombre de lemmes qu'il contient et la moyenne du nombre de fois où chaque lemme est utilisé.\n",
        "\n",
        "C'est un processus plus long, mais qui mélange tout ce qu'on a vu plus haut, qui mélange l'utilisation de `nltk` et de structures de données basiques en Python.\n",
        "\n",
        "### Étape 1 et 2\n",
        "D'abord, on **procède au nettoyage** et on **tokenise le texte**:"
      ],
      "metadata": {
        "id": "Ts-nYxw3cpcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt = lighthouse\n",
        "print(txt[:100])\n",
        "\n",
        "# 1)\n",
        "# `[^<motif>]` veut dire \"tout sauf <motif>\".\n",
        "# `[^a-z ]` veut donc dire \"tout ce qui n'est pas un espace ou un caractère alphabétique\"\n",
        "txt  = re.sub(\"[^a-z ]\", \" \", txt)  # que fait donc cette ligne?\n",
        "txt = re.sub(\"\\s+\", \" \", txt)       # on normalise les espaces\n",
        "print(txt[:100])\n",
        "\n",
        "# 2) on tokenise\n",
        "tokens = nltk.word_tokenize(txt)\n",
        "print(tokens[:100])\n",
        "print(len(tokens))"
      ],
      "metadata": {
        "id": "a1CuAVpvgcKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Étape 3\n",
        "Ensuite, **on supprime les stopwords**\n"
      ],
      "metadata": {
        "id": "qLDgHWVphydr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) on supprime les stopwords\n",
        "stop_words = list(stopwords.words(\"english\"))\n",
        "print(stop_words[:10])\n",
        "\n",
        "tokens_filtered = []\n",
        "for token in tokens:\n",
        "    if token.lower() not in stop_words:\n",
        "        tokens_filtered.append(token)\n",
        "print(tokens_filtered[:100])\n",
        "print(len(tokens_filtered))\n",
        "\n",
        "# en une ligne, comment calculer le nombre de tokens qui ont été supprimées?"
      ],
      "metadata": {
        "id": "YsQ5m84Xh3ER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Étape 4\n",
        "Pour continuer, on fait le **`pos tagging`et on crée notre corpus de 5000 mots-contenu**."
      ],
      "metadata": {
        "id": "8gVU658Aij4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "size = 5000  # la taille du corpus final: 5000 tokens\n",
        "sample_pos = []\n",
        "pos = nltk.pos_tag(tokens_filtered, tagset=\"universal\")  # part-of-speech tagging (classification du texte en classes: verbes...). universal définit des classes très généralistes\n",
        "for (token, tag) in pos:\n",
        "    if tag in [ \"NOUN\", \"VERB\", \"ADJ\", \"ADV\" ]:  # si ce token est un mot-contenu\n",
        "        sample_pos.append(token)\n",
        "sample_pos = random.sample(sample_pos, size)  # on créé une liste de 5000 items créés au hasard dans `sample_pos`\n",
        "\n",
        "print(sample_pos[:100])\n",
        "print(len(sample_pos))"
      ],
      "metadata": {
        "id": "BEPa7b08jlO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Étape 5\n",
        "\n",
        "**On lemmatise**. Comme pour le `pos tagging`, `nltk` propose des outils tout prêts, et plusieurs lemmatiseurs. On utilise le `WordNetLemmatizer`, entraîné sur le jeu de données [`WordNet`](https://wordnet.princeton.edu/)**texte en gras**"
      ],
      "metadata": {
        "id": "Mt9Y6Rq-j9ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "sample_lem = [ lemmatizer.lemmatize(token) for token in sample_pos ]  # exemple d'itération en une ligne: on boucle sur tous les items de `sample_pos` et on leur applique une fonction\n",
        "print(sample_lem)\n",
        "print(len(set(sample_lem)))  # le nombre de lemmes uniques dans notre liste de 5000 lemmes (`set` est un type de données équivalent à une ligne, mais sans doublons. utiliser `len()` sur une liste permet donc de supprimer les doublons de la `list`)"
      ],
      "metadata": {
        "id": "sBRh1apFkkKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Étape 6\n",
        "\n",
        "On calcule notre distribution de fréquences. C'est tout simple, comme on l'a déjà vu:"
      ],
      "metadata": {
        "id": "a4pnyV42lEXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fd = nltk.FreqDist(sample_lem)\n",
        "fd.tabulate()  # les 10 lemmes les plus utilisés\n",
        "\n",
        "fd.plot(title=\"To the lighthouse\")  # ça prend un peu de temps et c'est pas nécessairement ultra lisible"
      ],
      "metadata": {
        "id": "0LH2iKlklm35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Étape 7\n",
        "\n",
        "Comme on l'a vu avec le graphique ci-dessus, pour comparer les distributions de fréquences entre nos 3 romans, pour le moment, c'est pas donné: la courbe contient plusieurs milliers de valeurs, ce qui rend l'analyse à l'oeil nu difficile.\n",
        "\n",
        "On va donc **classer les lemmes en 10 groupes**, des 10% des lemmes les plus utilisés au 10% des lemmes les moins utilisés. C'est la partie la plus compliquée du code, mais elle ne comporte que des choses que l'on a déjà vues !"
      ],
      "metadata": {
        "id": "atphM9qQmqRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_moyenne = []  # [ <moyenne d'occurrences pour un lemme, par décile> ]\n",
        "distribution_somme = []    # [ <nombre absolu de lemmes, par décile> ]\n",
        "\n",
        "for i in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:  # on itère sur chaque décile\n",
        "    k = []  # liste du nombre d'occurrences pour chaque lemme, pour le décile actuel\n",
        "\n",
        "    # on remplit `k` avec le nombre d'occurrences par quartile\n",
        "    for mot, occurrences in fd.items():          # `fd.items()` permet de boucler sur des couples `<lemme>`: `<nombre d'occurrences>`\n",
        "        print(mot, occurrences)\n",
        "\n",
        "        quantile = occurrences/max(fd.values())  # représentation de l'utilisation du mot sur une échelle 0..1: 0 = mot jamais utilisé, 1 = mot le plus utilisé\n",
        "\n",
        "        # on vérifie si `quantile` correspond au quantile actuel `i`.\n",
        "        # si oui, on ajoute à `k`, notre liste du nombre d'occurrences,\n",
        "        # le nombre d'occurrences de ce lemme\n",
        "        if i != 1:\n",
        "            if i > quantile >= i-0.1:\n",
        "                k.append(occurrences)\n",
        "        else:\n",
        "            if i >= quantile >= i-0.1:\n",
        "                k.append(occurrences)\n",
        "\n",
        "    # on a fini de remplir `k` pour ce décile avec tous les lemmes du roman\n",
        "    # on calcule nos statistiques et on les ajoute aux listes `distribution`\n",
        "    if len(k) > 0:\n",
        "        mean = round(statistics.mean(k), 3)\n",
        "        distribution_moyenne.append(mean)\n",
        "    else:\n",
        "        distribution_moyenne.append(0)  # 0 mot ne rentre dans ce quantile => on ne peut calculer de moyenne\n",
        "    distribution_somme.append(len(k))\n",
        "\n",
        "print(distribution_somme)\n",
        "print(distribution_moyenne)\n",
        "print(distribution_moyenne[-1])  # comment interpréter ce nombre?\n",
        "print(distribution_somme[2])      # et celui-ci?"
      ],
      "metadata": {
        "id": "T6oy2XlYnPU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Et voilà ! Maintenant, on fait de tout ça une fonction, et ajoute les résultats à nos dictionnaires de statistiques."
      ],
      "metadata": {
        "id": "XTn6r1iMoxC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def distribution_vocabulaire(txt, stats, title):\n",
        "    \"\"\"\n",
        "    étudier la distribution du vocabulaire dans les trois romans\n",
        "    \"\"\"\n",
        "    # étape 1\n",
        "    txt = re.sub(\"[^a-z ]\", \" \", txt)  # on enlève tous les caractères non-alphabétiques et les espaces\n",
        "    txt = re.sub(\"\\s+\", \" \", txt)      # on normalise les espaces\n",
        "\n",
        "    # étape 2\n",
        "    tokens = nltk.word_tokenize(txt)   # tokenisation au mot (similaire à txt.split(\" \"), mais performe des simplifications en plus)\n",
        "\n",
        "    # étape 3\n",
        "    # on supprime tous les stopwords (mots jugés\n",
        "    # \"inutiles\" pour l'analyse automatique)\n",
        "    tokens_filtered = []\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    for token in tokens:\n",
        "        if token.lower() not in stop_words:\n",
        "            tokens_filtered.append(token)\n",
        "\n",
        "    # étape 4\n",
        "    # on fait un part-of-speech tagging sur le\n",
        "    # corpus pour pouvoir ensuite le lemmatiser\n",
        "    size = 5000  # la taille du corpus final: 5000 tokens\n",
        "    sample_pos = []\n",
        "    pos = nltk.pos_tag(tokens_filtered, tagset=\"universal\")  # part-of-speech tagging (classification du texte en classes: verbes...). universal définit des classes très généralistes\n",
        "    for (token, tag) in pos:\n",
        "        if tag in [ \"NOUN\", \"VERB\", \"ADJ\", \"ADV\" ]:\n",
        "            sample_pos.append(token)\n",
        "    sample_pos = random.sample(sample_pos, size)\n",
        "\n",
        "    # étape 5\n",
        "    # on lemmatise le corpus\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    sample_lem = [ lemmatizer.lemmatize(token) for token in sample_pos ]\n",
        "\n",
        "    # étape 6\n",
        "    # calculter une distribution de fréquences\n",
        "    fd = nltk.FreqDist(sample_lem)  # mot / nombres d'occurrences\n",
        "    fd.plot(title=title)\n",
        "\n",
        "    # étape 7\n",
        "    # grouper le vocabulaire en quantiles:\n",
        "    # lecture: les 10% des lemmes les plus fréquemment\n",
        "    # rencontrés sont utilisés en moyenne n fois.\n",
        "    distribution_moyenne = []  # [ <moyenne d'occurrences pour un lemme, par décile>]\n",
        "    distribution_somme = []    # [ <nombre absolu de lemmes, par décile>]\n",
        "    for i in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
        "        k = []  # liste du nombre d'occurrences par quantile\n",
        "        # on remplit `k` avec le nombre d'occurrences par quartile\n",
        "        for mot, occurrences in fd.items():\n",
        "            quantile = occurrences/max(fd.values())  # représentation de l'utilisation du mot sur une échelle 0..1: 0 = mot jamais utilisé, 1 = mot le plus utilisé\n",
        "            if i != 1:\n",
        "                if i > quantile >= i-0.1:\n",
        "                    k.append(occurrences)\n",
        "            else:\n",
        "                if i >= quantile >= i-0.1:\n",
        "                    k.append(occurrences)\n",
        "        # on calcule nos statistiques et on les ajoute aux listes `distribution`\n",
        "        if len(k) > 0:\n",
        "            mean = round(statistics.mean(k), 3)\n",
        "            distribution_moyenne.append(mean)\n",
        "        else:\n",
        "            distribution_moyenne.append(0)  # 0 mot ne rentre dans ce quantile => on ne peut calculer de moyenne\n",
        "        distribution_somme.append(len(k))\n",
        "\n",
        "    # enfin, on crée des dictionnaires pour afficher les résultats de façon lisible\n",
        "    deciles = [ \"0..10\", \"10..20\", \"20..30\", \"30..40\", \"40..50\"\n",
        "              , \"50..60\", \"60..70\", \"70..80\", \"80..90\", \"90..100\" ]\n",
        "    somme = {}\n",
        "    moyenne = {}\n",
        "    for i,d in enumerate(deciles):  # enumerate permet d'itérer sur une liste avec un couple un couple [index, valeur]\n",
        "        somme[d] = distribution_somme[i]\n",
        "        moyenne[d] = distribution_moyenne[i]\n",
        "    stats[\"lemmes distincts\"] = somme\n",
        "    stats[\"lemme moyenne\"] = moyenne\n",
        "    return stats\n",
        "\n",
        "stats_lighthouse = distribution_vocabulaire(lighthouse, lighthouse_stats, \"To the lighthouse\")\n",
        "stats_dalloway = distribution_vocabulaire(dalloway, dalloway_stats, \"Mrs. Dalloway\")\n",
        "stats_waves = distribution_vocabulaire(waves, stats_waves, \"The Waves\")\n"
      ],
      "metadata": {
        "id": "1GmSvAwEpDFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Js6R4_-To5wA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Afficher les résultats définitifs\n",
        "\n",
        "Ici encore, on utilise `tabulate`. Je rentre pas dans le détail de la fonction `.join()`. Elle permet de joindre les différentes valeurs d'un itérable par une chaîne de caractères, ici `\\n`."
      ],
      "metadata": {
        "id": "BrWzeYrprUFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headers = [ \"\", \"Mrs. Dalloway, 1925\", \"To the Lighthouse, 1927\", \"The Waves, 1931\" ]\n",
        "data = [ [ \"nombre médian de mots par paragraphes\"\n",
        "         , stats_dalloway[\"nombre médian de mots par paragraphes\"]\n",
        "         , stats_lighthouse[\"nombre médian de mots par paragraphes\"]\n",
        "         , stats_waves[\"nombre médian de mots par paragraphes\"]\n",
        "         ],\n",
        "         [ \"nombre médian de phrases par paragraphes\"\n",
        "         , stats_dalloway[\"nombre médian de phrases par paragraphes\"]\n",
        "         , stats_lighthouse[\"nombre médian de phrases par paragraphes\"]\n",
        "         , stats_waves[\"nombre médian de phrases par paragraphes\"]\n",
        "         ],\n",
        "         [ \"nombre médian de mots par phrase\"\n",
        "         , stats_dalloway[\"nombre médian de mots par phrase\"]\n",
        "         , stats_lighthouse[\"nombre médian de mots par phrase\"]\n",
        "         , stats_waves[\"nombre médian de mots par phrase\"]\n",
        "         ],\n",
        "         [ \"nombre moyen de signes de ponctuation par phrase\"\n",
        "         , stats_dalloway[\"nombre moyen de signes de ponctuation par phrase\"]\n",
        "         , stats_lighthouse[\"nombre moyen de signes de ponctuation par phrase\"]\n",
        "         , stats_waves[\"nombre moyen de signes de ponctuation par phrase\"]\n",
        "         ],\n",
        "         [ \"densité lexicale\"\n",
        "         , stats_dalloway[\"densité lexicale\"]\n",
        "         , stats_lighthouse[\"densité lexicale\"]\n",
        "         , stats_waves[\"densité lexicale\"]\n",
        "         ],\n",
        "         [ \"distribution du vocabulaire\\n(nombre de lemmes distincts par décile)\"\n",
        "         , \"\\n\".join(f\"{k} : {v}\" for k,v in stats_dalloway[\"lemmes distincts\"].items() )\n",
        "         , \"\\n\".join(f\"{k} : {v}\" for k,v in stats_lighthouse[\"lemmes distincts\"].items() )\n",
        "         , \"\\n\".join(f\"{k} : {v}\" for k,v in stats_waves[\"lemmes distincts\"].items() )\n",
        "         ],\n",
        "         [ \"distribution du vocabulaire\\n(moyenne d'utilisation d'un lemme par décile)\"\n",
        "         , \"\\n\".join(f\"{k} : {v}\" for k,v in stats_dalloway[\"lemme moyenne\"].items() )\n",
        "         , \"\\n\".join(f\"{k} : {v}\" for k,v in stats_lighthouse[\"lemme moyenne\"].items() )\n",
        "         , \"\\n\".join(f\"{k} : {v}\" for k,v in stats_waves[\"lemme moyenne\"].items() )\n",
        "         ]\n",
        "]\n",
        "print(tabulate(data, headers, tablefmt=\"rounded_grid\"))\n",
        ""
      ],
      "metadata": {
        "id": "ga-uzdK9rs74"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}